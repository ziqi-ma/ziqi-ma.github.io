<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ziqi-ma.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ziqi-ma.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-07T05:06:07+00:00</updated><id>https://ziqi-ma.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Find3D: Localizing Semantic Concepts in the 3D Space</title><link href="https://ziqi-ma.github.io/blog/2025/find3d/" rel="alternate" type="text/html" title="Find3D: Localizing Semantic Concepts in the 3D Space"/><published>2025-08-06T21:15:00+00:00</published><updated>2025-08-06T21:15:00+00:00</updated><id>https://ziqi-ma.github.io/blog/2025/find3d</id><content type="html" xml:base="https://ziqi-ma.github.io/blog/2025/find3d/"><![CDATA[<p>Intelligence in the virtual world can live in the form of language, yet the physical world is different – it is described by physical coordinates of x,y, and z. To build a bridge between the two, in the simplest sense, means being able to pinpoint where in (x,y,z) a semantic concept corresponds to. I believe that localizing semantic concepts in space is the first step towards enabling physical intelligence. This is what <a href="https://ziqi-ma.github.io/find3dsite/">Find3D</a> sets out to solve.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/teaser-480.webp 480w,/assets/img/find3dblog/teaser-800.webp 800w,/assets/img/find3dblog/teaser-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/find3dblog/teaser.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> (Generated by GPT-4o w/ human editing) </div> <h2 id="constraining-the-solution-space---key-design-decisions">Constraining the solution space - key design decisions</h2> <p>Of course, localization cannot be solved in one single project. We make a few design decisions to focus on the most critical setting:</p> <ol> <li>Work on the point cloud representation: point clouds are easily accessible with depth sensors in robotic applications, and <a href="https://arxiv.org/abs/2503.11651">VGGT-type methods</a> yield point maps easily. </li> <li>Focus on object parts - manipulation-type tasks require understanding below the object level - e.g. lifting the handle. Parts are closely related to affordance, yet visual understanding beyond the object granularity has been much less studied than object-level understanding. </li> <li>Semantic localization should be 3D-native rather than 2D-lifted. Why? <ol type="a"> <li>Multiview camera systems cannot scale to in-the-wild settings, moving the camera cannot scale to dynamic scenes, and single view usually contains occlusions.</li> <li>Progress in 3D reconstruction has been accelerating (such as <a href="https://arxiv.org/abs/2412.01506">Trellis</a>, <a href="https://arxiv.org/abs/2503.11651">VGGT</a>, <a href="https://arxiv.org/abs/2502.12894">CAST</a>), so obtaining 3D is no longer the bottleneck.</li> </ol> </li> <li>We embrace <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">the bitter lesson</a>, and want to go for a scalable data strategy and a scalable architecture that can keep leveraging the growing data.anularity has been much less studied than object-level understanding. </li> </ol> <h2 id="the-data-strategy">The data strategy</h2> <p>Now we know that we want a 3D native model for localizing concepts on the part level, and one that works generally (rather on a toy set of objects). The question is how. While we have small benchmarks and datasets for segmentation such as <a href="https://cs.stanford.edu/~ericyi/project_page/part_annotation/">ShapeNetPart</a> and <a href="https://partnet.cs.stanford.edu/">PartNetE</a>, they are very constrained in their domains. We do not have annotated data for general objects. As shown in the figure below, the data volume in 3D is in no way comparable with 2D, not to mention language.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/data_scale-480.webp 480w,/assets/img/find3dblog/data_scale-800.webp 800w,/assets/img/find3dblog/data_scale-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/find3dblog/data_scale.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>While we have a data shortage in 3D, we do have abundant data sourced from the Internet, which has led to incredibly powerful <a href="https://arxiv.org/abs/2410.21276">VLMs</a> with visual understanding capabilities in 2D. I realized that actually we can turn the localization problem into a recognition problem by building a data engine.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/dataengine-480.webp 480w,/assets/img/find3dblog/dataengine-800.webp 800w,/assets/img/find3dblog/dataengine-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/find3dblog/dataengine.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>The data engine starts by rendering unlabeled online 3D assets from Objaverse, and obtaining masks by grid-prompting SAM. We want to highlight that existing 2D “part finding” models such as groundedSAM usually misses parts, and thus by grid-prompting we can catch more comprehensive parts, especially ones that are small or seen in less common viewpoints. The goal here is not to have complete coverage for each individual object, but rather to cover a wide range of objects and parts over the full dataset. With proper filtering, we obtain a total of around 200 masks per object (from 10 views). In the next section, I will discuss how to leverage such powerful (but noisy) data.</p> <p>By highlighting parts with colored masks, we can pass these overlaid renderings to VLMs such as Gemini to recognize the name of the highlighted part, and use a CLIP-like embedding of the name as the true label for all corresponding points. These masks then get back-projected to 3D to serve as point-level annotations. Because we use language as a medium, we naturally yield diverse labels in 2 senses:</p> <ol> <li>The language description is diverse, e.g. the body of the telescope is both called a “telescope tube” and “body of a telescope”.</li> <li>We get multiple levels of granularity, rather than adhering to one set of manually-defined criteria: e.g. we get “base” of a milkshake as well as the coarser-granularity “milkshake glass”.</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/dataengine_qualitative_website-480.webp 480w,/assets/img/find3dblog/dataengine_qualitative_website-800.webp 800w,/assets/img/find3dblog/dataengine_qualitative_website-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/find3dblog/dataengine_qualitative_website.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 90%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <h2 id="training-and-scaling-the-model">Training and scaling the model</h2> <p>The data from our data engine is very diverse, yet also noisy. Two main challenges in training are seen in the image below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/challenges_training-480.webp 480w,/assets/img/find3dblog/challenges_training-800.webp 800w,/assets/img/find3dblog/challenges_training-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/find3dblog/challenges_training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 70%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <ol> <li>As much as we try to make our labels dense, there are points that cannot be covered by our viewpoints/selected masks. We need to find ways to also provide supervision on those points.</li> <li>Because our labels might come from different granularities or come under different descriptions, each point could fall under different labels (either from the same view or from overlapping views). We need to reconcile that.</li> </ol> <p>In computer vision, we have a long history of dealing with large-scale, noisy data. We follow a contrastive approach as shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/contrastive-480.webp 480w,/assets/img/find3dblog/contrastive-800.webp 800w,/assets/img/find3dblog/contrastive-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/find3dblog/contrastive.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>For each (asset, label) pair from the data engine, we pair the pooled point features corresponding to the label’s backprojected points with the actual label feature as a positive pair. We aggregate over objects to get around 3k positive pairs per batch. Architecturally, we adopt the PT3 architecture to be able to learn geometric prior over points, as shown below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/method-480.webp 480w,/assets/img/find3dblog/method-800.webp 800w,/assets/img/find3dblog/method-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/find3dblog/method.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>With this formulation, we can address the two challenges mentioned above:</p> <ol> <li>For unannotated points, given geometric priors learned by our model, they naturally share similar features with points that have similar geometric properties (i.e. points on the same sphere that are annotated).</li> <li>For points with multiple labels, their feature is pushed towards all of the supervision labels, so that at inference time we support querying at different granularities or with language descriptions emphasizing various aspects like function or material.</li> </ol> <p>The contrastive objective helps with learning from our large-scale, diverse data. We also observe good scaling behavior as we increase the number of object categories in the training data, as shown in the figure below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/scaling-480.webp 480w,/assets/img/find3dblog/scaling-800.webp 800w,/assets/img/find3dblog/scaling-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/find3dblog/scaling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 50%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <h2 id="achieving-generalization">Achieving generalization</h2> <p>Find3D is able to locate parts on diverse objects, and can even localize challenging parts (rightmost column), such as parts that are difficult to locate in 2D – the light bulb inside the lampshade (occluded almost in every viewing direction), the rim of a mug, or the sleeve of a chromatic shirt (no edge delineation).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/qualitative_new-480.webp 480w,/assets/img/find3dblog/qualitative_new-800.webp 800w,/assets/img/find3dblog/qualitative_new-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/find3dblog/qualitative_new.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 90%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>Even on smaller datasets that other methods train on, we achieve better results zero-shot in addition to being much faster.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/quantitative_res-480.webp 480w,/assets/img/find3dblog/quantitative_res-800.webp 800w,/assets/img/find3dblog/quantitative_res-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/find3dblog/quantitative_res.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>More interestingly, we are able to generalize into the real world in ways that the more recent decomposed generation methods cannot, as shown below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/partpacker_comp-480.webp 480w,/assets/img/find3dblog/partpacker_comp-800.webp 800w,/assets/img/find3dblog/partpacker_comp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/find3dblog/partpacker_comp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <h2 id="whats-next">What’s next?</h2> <p>Going back to the initial motivation, we want to solve semantic part localization because it is the first step towards bridging the current intelligence in language towards the physical space delineated by spatial coordinates x,y,z. A natural next step is to make it really useful in end-to-end robotic tasks, and tackling the more general problem of robotic/reasoning interface. Stay tuned!</p>]]></content><author><name></name></author><category term="blogs"/><category term="AI"/><summary type="html"><![CDATA[Intelligence in the virtual world can live in the form of language, yet the physical world is different – it is described by physical coordinates of x,y, and z. To build a bridge between the two, in the simplest sense, means being able to pinpoint where in (x,y,z) a semantic concept corresponds to. I believe that localizing semantic concepts in space is the first step towards enabling physical intelligence. This is what Find3D sets out to solve.]]></summary></entry><entry><title type="html">3D &amp;amp; The Bitter Lesson – CVPR25 Rambling Thoughts</title><link href="https://ziqi-ma.github.io/blog/2025/cvpr25/" rel="alternate" type="text/html" title="3D &amp;amp; The Bitter Lesson – CVPR25 Rambling Thoughts"/><published>2025-07-05T14:15:00+00:00</published><updated>2025-07-05T14:15:00+00:00</updated><id>https://ziqi-ma.github.io/blog/2025/cvpr25</id><content type="html" xml:base="https://ziqi-ma.github.io/blog/2025/cvpr25/"><![CDATA[<p>One of the implications of the bitter lesson is that scaling up data and computation is far more effective than hand-designing structures. In this sense, 3D vision is a bit weird - for any end task (robotics, AR/VR, design), we do not have easily-accessible large-scale data. When we talk about 3D vision, or 3D perception, by definition we are defining a specific modular breakdown of an end application. In <a href="https://drive.google.com/file/d/1VodGljuEhBKwZIXQwN-ApH6g2wBAVAdK/view">Ross Girschick’s words</a>, is 3D perception a “parser”? Or in <a href="https://x.com/jon_barron/status/1916918872177054171">Jon Barron’s words</a>, has generative video “bitter-lessoned” 3D?</p> <p><strong>How to reconcile the bitter lesson with the data scarcity of 3D?</strong> With this big question in mind, I arrived at CVPR25 in Nashville, hoping to get more perspectives from the community. Numerous chats, posters, and workshops later, I found myself extremely excited about two answers to the above question. Since we do not have large-scale data for any 3D application for free, we have two options:</p> <ul> <li>A. Embrace some modularity - e.g. separate out perception priors (about object, motion, or physics) into a vision module that is pretrained separately from E2E tasks</li> <li>B. Build data engines that can leverage new signals that complement our existing datasets</li> </ul> <h2 id="why-3d-understanding-tasks-are-not-parsers">Why 3D understanding tasks are not “parsers”</h2> <p>While the learning from LLMs is that large-scale E2E training &gt;&gt; human-designed structure, I do think applications like robotics could benefit from some modularity with a good perception module that needn’t be bottlenecked by the scarcity of action data.</p> <p>I see all of 3D understanding tasks as ways to find the right representation of the observed world. The question is whether such representations are best learned from scaled-up robot action data or should be improved via strong visual pretraining. Both sides gave interesting opinions at the Generalization in Robotics Manipulation workshop. On the one hand, Chelsea Finn mentioned that long video sequences in robotic data already implicitly encodes information, such as 3D. On the other hand, for people who currently favor more modular approaches (<a href="https://arxiv.org/pdf/2506.01185">HoMeR</a>, <a href="https://arxiv.org/abs/2405.01527">Track2Act</a>), they wish to pursue stronger, more informative visual representations and much more data-efficient action generalization. I assume that both eventually would help us build stronger VLA models. As shown by Gemini, a good VLM backbone pre-trained on 3D tasks such as detection and multiview correspondence (<a href="https://arxiv.org/abs/2503.20020">Gemini Robotics ER</a>) helps the E2E action model improve both performance and generalization.</p> <p>I do believe that we should take advantage of all the vision data and 3D understanding tasks we have to obtain the best representation – an understanding of shapes, motion, and common sense. The end goal is probably to integrate all these tasks/representations into either pretraining tasks or the vision encoder – as earlier results in language (e.g. <a href="https://arxiv.org/abs/2109.01652">FLAN</a>) have shown, such mixing of tasks could improve both performance on existing tasks and generalization to new tasks. Along the way, we could also learn new insights about what training recipes are scalable and efficient, and gain new insights about architecture. In this sense, I do think it is valuable to work on modular visual understanding tasks and to have standalone metrics. The true lesson from language, in my opinion, is the discovery that the autoregressive generation pretraining task benefits all downstream E2E tasks, rather than solely focusing on specific E2E tasks such as translation or summarization. Before the GPT results, it was unclear that this specific unified formulation improves E2E performance of all downstream tasks, yet people spent the time and effort investigating it because they believed in the generality of the autoregressive formulation. Thus, in addition to “don’t build a parser”, I also want to say we should not “focus only on the translation metrics” but focus on finding general representations and tasks. I think the current investigation in all 3D understanding (grounding/text-3D/motion), albeit each having its own metric, helps us in the quest of a general visual representation of the world. Eventually the validity of such representations might only be measurable on large-scale projects (think GPT), but the technology buildup towards that final evaluation needs to happen gradually and even perhaps modularly.</p> <h2 id="data-engines-look-out-for-new-signals-that-enrich-existing-data">Data engines: look out for new signals that enrich existing data</h2> <p>As we are closer than ever to exhausting our available data sources, we need to start seeking other types of signals, either from humans (in less labor-intensive ways) or from other models. I define data engines broadly as systems composed of one or more models, with the goal of enriching current data with new signals. The new signals can fall broadly in two buckets: feedback, or more diverse data sources.</p> <p>The general idea of incorporating feedback into training is nothing new: that is how we initially achieved text-to-3D (<a href="https://arxiv.org/abs/2209.14988">DreamFusion</a>), how we accelerated supervision for foundation models like <a href="https://arxiv.org/abs/2304.02643">SAM</a>, and how we now approach LLM alignment (<a href="https://arxiv.org/abs/2203.02155">RLHF</a>) and reasoning (<a href="https://arxiv.org/abs/2411.15124">RLVR</a>). These are usually one-step approaches that leverage zero or one models, but I believe there is so much more room – both in coming up with feedback more creatively, and in applying it to new domains and tasks. I came across one such formulation for robotics from a workshop talk given by Ranjay Krishna. The data engine is composed of a base model and a verifier. The verifier (<a href="https://arxiv.org/abs/2410.00371">AHA</a> in this case) can detect robotic failure in each stage of a potentially long-horizon task. The base model rolls out a policy, the verifier verifies, and if all stages pass, the policy is added to the dataset. Without resetting the robot, a VLM proposes the next goal, and the model repeats this rollout + verification loop, theoretically infinitely. With such a verifier, we can keep “distilling” suboptimal generator models to better ones. One could also imagine that once the base model is good enough, the verifier can be directly used for RLVR.</p> <p>At this CVPR25, there was also a lot of excitement about enriching the 3D data source via reconstruction methods (e.g. <a href="https://arxiv.org/abs/2503.11651">VGGT</a>). Such data can be used directly to train self-supervised representations such as <a href="https://xywu.me/sonata/">Sonata</a>, or combined with other foundation models to provide data for understanding tasks (such as what I tried in my work, <a href="https://ziqi-ma.github.io/find3dsite/">Find3D</a>).</p> <p>The data limitation in 3D forces us to think harder – since it is not easy to train everything E2E on large data, we need to get more creative. I am extremely excited about both paths forward: making 3D perception a bit modular and “pretrainable” to save on E2E task data, as well as exploring systems that can leverage new signals to enrich our existing data.</p>]]></content><author><name></name></author><category term="blogs"/><category term="AI"/><summary type="html"><![CDATA[One of the implications of the bitter lesson is that scaling up data and computation is far more effective than hand-designing structures. In this sense, 3D vision is a bit weird - for any end task (robotics, AR/VR, design), we do not have easily-accessible large-scale data. When we talk about 3D vision, or 3D perception, by definition we are defining a specific modular breakdown of an end application. In Ross Girschick’s words, is 3D perception a “parser”? Or in Jon Barron’s words, has generative video “bitter-lessoned” 3D?]]></summary></entry><entry><title type="html">The emerging paradigm of autoregressive VLMs</title><link href="https://ziqi-ma.github.io/blog/2024/multimodal/" rel="alternate" type="text/html" title="The emerging paradigm of autoregressive VLMs"/><published>2024-11-26T11:38:00+00:00</published><updated>2024-11-26T11:38:00+00:00</updated><id>https://ziqi-ma.github.io/blog/2024/multimodal</id><content type="html" xml:base="https://ziqi-ma.github.io/blog/2024/multimodal/"><![CDATA[<p>When the <a href="https://arxiv.org/abs/2108.07258">“foundation models” paper</a> first used the term “paradigm shift” in 2021, I remember chuckling at how self-important the term seemed. Today in the end 2024, I am feeling the full gravity of the shift from building problem-specific custom solutions to building upon a common core component that can scale. I think this change deserves to be called a “paradigm shift”, true to Kuhn’s original intent for the term in 1962. This shift is enabled by the emergence of unified task interfaces and the convergence of model architecture. As a result, a framework of execution is also starting to consolidate, with a focus on data acquisition and systematic upscaling. I discuss each aspect in order.</p> <p><br/></p> <h2 id="the-unifying-task-interface-of-autoregressive-generation">The unifying task interface of autoregressive generation</h2> <p>The early versions of modern, large-scale VLMs are trained contrastively (<a href="https://arxiv.org/abs/2103.00020">CLIP</a>), perhaps because image-text pairs are the easiest type of supervision that we can obtain at scale. Soon we realized that making image and text features similar is not sufficient. With a similarity comparison we can only do retrieval and classification, but we dream of something more – features from another modality that is not only similar but also usable – to reason, to generate, to answer questions. These tasks can all be unified under the autoregressive generation framework – and thus, trained on the same language modeling loss. Earlier models like <a href="https://arxiv.org/abs/2205.01917">CoCa</a>, <a href="https://arxiv.org/abs/2301.12597">BLIP2</a>, <a href="https://arxiv.org/abs/2111.08276">X-VLM</a> use a hybrid of contrastive and language modeling objectives. The common benchmarks (<a href="https://arxiv.org/abs/1612.00837">VQAV2</a>, <a href="https://lil.nlp.cornell.edu/nlvr/">NLVR2</a>) are old, yet the unification of tasks is new. Later models (<a href="https://arxiv.org/abs/2204.14198">Flamingo</a>, <a href="https://arxiv.org/abs/2209.06794">PALI family</a>, <a href="https://arxiv.org/abs/2208.10442">BEiT3</a>, <a href="https://arxiv.org/abs/2304.08485">Llava</a> etc.) discard the contrastive component and fully use the language modeling objective. While one might argue that putting language-centric tasks into an autoregressive framework is nothing surprising, we also see an increased coverage of more vision-centric tasks.</p> <p>From the early exploration of <a href="https://arxiv.org/abs/2102.12092">DALLE-1</a>, several lines of work (<a href="https://sites.research.google/parti/">Parti</a> – perhaps <a href="https://arxiv.org/abs/2312.11805">Gemini</a>, <a href="https://arxiv.org/abs/2405.09818">Chameleon</a> family) also formulate image generation as an autoregressive task under the same interface. Although diffusion is still the leading solution in many settings, it is not as unified with all the other tasks, and people seem to be still pushing on autoregressive generation.</p> <p>Denser vision tasks are first formulated as more fine-grained language tasks, such as <a href="https://arxiv.org/abs/2210.01936">ARO</a>, <a href="https://arxiv.org/abs/2207.00221">VL-Checklist</a> and <a href="https://arxiv.org/abs/2204.03162">Winoground</a>. Such tasks seem more as an evaluation (or a highlight of current failure modes) and not training tasks. However, more recently, classical dense vision tasks like detection and segmentation are also formulated autoregressively with additional tokens denoting boxes and masks. These tasks are incorporated into pre-training for models like the PALI family, yielding decent performance and enabling considerable improvement on a variety of tasks. <a href="https://arxiv.org/abs/2406.16860">Cambrian1</a> repurposed most vision tasks into a language framework, including detection, segmentation, and depth (formulated as comparison). Formulating dense vision tasks autoregressively (especially the segmentation masks which first start with polygon coordinates then use the <a href="https://arxiv.org/abs/1711.00937">VQ-VAE</a> encoding) is mind-blowing for me, yet the performance seems okay. More recently, <a href="https://arxiv.org/abs/2409.17146">Molmo</a> releases a “pointing” feature where a VLM can answer a question by “pointing” in an image.</p> <div class="row mt-4"> <div class="col-sm mt-5 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vlmblog/paligemma-480.webp 480w,/assets/img/vlmblog/paligemma-800.webp 800w,/assets/img/vlmblog/paligemma-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/vlmblog/paligemma.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-5 mt-md-0"> <div style="display: flex; margin: auto; justify-content: center; vertical-align: middle;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vlmblog/molmo-480.webp 480w,/assets/img/vlmblog/molmo-800.webp 800w,/assets/img/vlmblog/molmo-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/vlmblog/molmo.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> How PaliGemma and Molmo formulate dense vision tasks autoregressively. </div> <p>A unified interface is in no ways prescriptive of all future models. However, it proposes one way to push ahead on scale, and one way to let the “divide, iterate, conquer” kick off. For example, <a href="https://arxiv.org/abs/2406.16860">Cambrian1</a> uses a common pipeline to measure the effectiveness of various types of visual encoders. While the comparison certainly comes with its bias, I think this type of interface abstraction is the right way for a large team to make progress on a large foundation model.</p> <p><br/></p> <h2 id="the-architecture-convergence">The architecture convergence</h2> <p>Architecture co-develops with the tasks. It is hard to say which causes which - the unification of tasks under the autoregressive framework, or the scalability of the decoder architecture. Since the output eventually is tokens (language, image or special tokens), the question becomes how to map raw image input to the token space. A few attempts have been cross-attention (<a href="https://arxiv.org/abs/2204.14198">Flamingo</a>), adapter with learnable queries (<a href="https://arxiv.org/abs/2204.14198">Flamingo</a>, <a href="https://arxiv.org/abs/2301.12597">BLIP-2</a> QFormer), or simple projection (<a href="https://arxiv.org/abs/2407.07726">PaliGemma</a>). For the early-fusion models (<a href="https://sites.research.google/parti/">Parti</a>, <a href="https://arxiv.org/abs/2405.09818">Chameleon</a> family), the visual signal is tokenized via some self-supervised discrete tokenizer such as <a href="https://arxiv.org/abs/1711.00937">VQ-VAE</a> or <a href="https://arxiv.org/abs/2303.13450">Set-a-Scene</a>.</p> <p>A natural decomposition becomes an image encoder, an adapter mechanism (one of the above three) which maps the image into a “soft prompt”, and a language decoder. Earlier works have attempted to train these encoders from scratch (<a href="https://arxiv.org/abs/2205.01917">CoCa</a>, <a href="https://arxiv.org/abs/2111.08276">X-VLM</a>, <a href="https://arxiv.org/abs/2208.10442">BEiT3</a>), yet later works (<a href="https://arxiv.org/abs/2209.06794">PALI</a> family, <a href="https://arxiv.org/abs/2405.09818">Chameleon</a>, <a href="https://arxiv.org/abs/2112.04482">Flava</a>, etc.) mainly leverage existing models as encoders, partly due to the increased size of these models. Somewhat surprisingly, CLIP-type image-text contrastively learned vision encoders seem to work better than vision-only encoders (<a href="https://arxiv.org/abs/2406.16860">Cambrian1</a>), even on vision-centric tasks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vlmblog/paligemma_arch-480.webp 480w,/assets/img/vlmblog/paligemma_arch-800.webp 800w,/assets/img/vlmblog/paligemma_arch-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/vlmblog/paligemma_arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> Architecture of PaliGemma. The architectures of autoregressive VLMs seem to simplify over time. Not much bells and whistles, just a vision encoder, a language encoder, then autoregressive generation. Usually pretrained encoders are used. For image generation, latent tokens are usually first generated, then mapped back to pixel space. </div> <p>The question on architecture, under the autoregressive framework’s emerging interfaces, now becomes “how to encode and tokenize image”. This is a much more constrained design space, yet one that allows for relatively smooth interface with all the existing language architecture and the tasks that have been subsumed under this framework. Any new model proposal can be easily plugged in and swapped out.</p> <p><br/></p> <h2 id="data">Data</h2> <p>As the architectures are converging, attention has been turning to data and pretraining task mixture. To improve performance in specific domains like OCR or dense visual tasks like detection and segmentatino, such tasks are sometimes directly added into the pretraining mix. While re-mixing existing data is a quick fix, we really do need to think hard about what new kinds of data can enable new capabilities or big improvements.</p> <p>Throughout the history of AI, data has usually co-developed with the architecture. On the one hand, the availability of supervision at scale greatly determines what models are possible to train. For example, image-text pairs naturally gave rise to contrastive models. Interleaved HTML text (<a href="https://arxiv.org/abs/2201.07520">CM3</a>) at scale enables mixed-modality output models. Dense datasets like <a href="https://cocodataset.org/#home">COCO</a> (and the plethora of datasets built upon it) enable models that have any ability to reason in a denser/object-centric manner. On the other hand, smaller-scale POCs determine what types of data is worth investing on, as one can see in the lines of industrial work that either get scaled up or become discontinued.</p> <p>One good example of new form of data is <a href="(https://arxiv.org/abs/2409.17146)">PixMo-Points</a>. The point dataset with 2.3M Q&amp;A pairs and 428k images enables Molmo’s new pointing feature. Their data collection via speaking and automatic speech-to-text transcription also greatly improves annotation efficiency. Meta also released a dense, mask-level annotated dataset called <a href="https://arxiv.org/abs/2312.08578">DCI</a> this summer, although only at a 7k-image scale. Such a dataset, if scaled up, has the potential of greatly improving grounding.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vlmblog/data-480.webp 480w,/assets/img/vlmblog/data-800.webp 800w,/assets/img/vlmblog/data-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/vlmblog/data.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> Examples of dense, mask-level captions from Meta's DCI dataset. This is exactly the kind of information that humans can get from the visual modality that would be so inconvenient to convey in language. These captions are valuable for grounding tasks, although we currently only have 7k images openly available. </div> <p><br/></p> <h2 id="what-kind-of-mechanistic-investigations-do-we-need">What kind of mechanistic investigations do we need</h2> <p>With much of the community interested in the common goal of building a good multitask VLM, there is an increasing need of systematic understanding of what might work and what might not. While in old-fashioned ML works there is usually a small section reserved for theoretical derivation that is supposed to serve this purpose, in the current day we need a different kind of mechanistic understanding. The focus should be to guide the investment of time and compute, understand what scales and what does not. Good examples include <a href="https://arxiv.org/abs/2405.02246">Idefics2</a> and <a href="https://arxiv.org/abs/2406.16860">Cambrian1</a>, which define and explore the design space and scaling laws, so that not everyone needs to repeat the same experiments before making their own design decisions. While seeing so much emphasis on scaling in research is surprising, this heralds that something is on the horizon and concerted effort is forming within the community.</p> <p><br/></p> <h2 id="whats-next">What’s next?</h2> <p>The forming of one set of interfaces does not mean we already nailed down most of the problems. On the contrary, it formulates one design space which can help guide us in tackling future problems in a more structured way.</p> <ul> <li>While the autoregressive framework has promise to scale, this does not mean exploration on other fronts e.g. diffusion should stop. Perhaps one could think more along the lines of unification of the two. Especially, diffusion strikes me as a much more natural way to model dense tasks.</li> <li>Going sub-object level – the main difference between the image and the language modality, in my opinion, is the rich details that the vision modality embodies. Up to the object level, one can imagine some level of abstraction into a much lower-dimension token space, yet we can perceive sub-object details on different levels of granularity, depending on the context. This is not yet well-formulated either in task or in architecture. The current way of tokenizing might or might not be sufficient.</li> <li>What are good ways to encode 3D data (for any representation), and what are good task formulations that can represent the needs for embodied applications well, while remaining compatible to the existing scalable framework?</li> </ul> <p>To these ends, I did my own investigation on generating dense semantic features for 3D point clouds, called <a href="https://arxiv.org/abs/2411.13550">Find3D</a>. Segmentation/localization is a straightforward use case. How to leverage this now-acquired dense knowledge to enable better reasoning? How does this representation fit into the broader VLM frameworks? These are questions that remain to be solved.</p> <p>In a sense, with the interfaces emerging, the line between free research and product roadmaps become more blurred. Coordinated, large-scale efforts are now possible. We are seeing enough initial evidence that if researchers combine efforts and work under compatible interfaces, our collective impacts could add up to something quite exciting!</p>]]></content><author><name></name></author><category term="blogs"/><category term="AI"/><summary type="html"><![CDATA[When the “foundation models” paper first used the term “paradigm shift” in 2021, I remember chuckling at how self-important the term seemed. Today in the end 2024, I am feeling the full gravity of the shift from building problem-specific custom solutions to building upon a common core component that can scale. I think this change deserves to be called a “paradigm shift”, true to Kuhn’s original intent for the term in 1962. This shift is enabled by the emergence of unified task interfaces and the convergence of model architecture. As a result, a framework of execution is also starting to consolidate, with a focus on data acquisition and systematic upscaling. I discuss each aspect in order.]]></summary></entry><entry><title type="html">Many facets of un-truth: LLM hallucination 101</title><link href="https://ziqi-ma.github.io/blog/2023/llm101/" rel="alternate" type="text/html" title="Many facets of un-truth: LLM hallucination 101"/><published>2023-09-05T16:40:16+00:00</published><updated>2023-09-05T16:40:16+00:00</updated><id>https://ziqi-ma.github.io/blog/2023/llm101</id><content type="html" xml:base="https://ziqi-ma.github.io/blog/2023/llm101/"><![CDATA[<p>Check out this new article I wrote on Data Science@Microsoft medium blog: <a href="https://medium.com/data-science-at-microsoft/many-facets-of-un-truth-llm-hallucination-101-8273806fa145">Many facets of un-truth: LLM hallucination 101</a>. This is a 101 on hallucination that I wish I had when getting started with LLMs. Hope it could be helpful to you too!</p>]]></content><author><name></name></author><category term="blogs"/><category term="AI"/><summary type="html"><![CDATA[Check out this new article I wrote on Data Science@Microsoft medium blog: Many facets of un-truth: LLM hallucination 101. This is a 101 on hallucination that I wish I had when getting started with LLMs. Hope it could be helpful to you too!]]></summary></entry><entry><title type="html">Reasoning about change: Lessons learned from building a near real-time system for Azure pricing</title><link href="https://ziqi-ma.github.io/blog/2023/streaming/" rel="alternate" type="text/html" title="Reasoning about change: Lessons learned from building a near real-time system for Azure pricing"/><published>2023-02-05T16:40:16+00:00</published><updated>2023-02-05T16:40:16+00:00</updated><id>https://ziqi-ma.github.io/blog/2023/streaming</id><content type="html" xml:base="https://ziqi-ma.github.io/blog/2023/streaming/"><![CDATA[<p>Check out this new article I wrote on Data Science@Microsoft medium blog: <a href="https://medium.com/data-science-at-microsoft/reasoning-about-change-lessons-learned-from-building-a-near-real-time-system-for-azure-pricing-34049816ffbd1">Reasoning about change: Lessons learned from building a near real-time system for Azure pricing</a>. The pricing projects led me to read up on the development of various industrial streaming systems in the past decade. Such a journey from POC to industry standard - quite inspiring!</p>]]></content><author><name></name></author><category term="blogs"/><category term="Engineering"/><summary type="html"><![CDATA[Check out this new article I wrote on Data Science@Microsoft medium blog: Reasoning about change: Lessons learned from building a near real-time system for Azure pricing. The pricing projects led me to read up on the development of various industrial streaming systems in the past decade. Such a journey from POC to industry standard - quite inspiring!]]></summary></entry><entry><title type="html">Speeding up “Reverse ETL”</title><link href="https://ziqi-ma.github.io/blog/2022/reverseETL/" rel="alternate" type="text/html" title="Speeding up “Reverse ETL”"/><published>2022-12-05T16:40:16+00:00</published><updated>2022-12-05T16:40:16+00:00</updated><id>https://ziqi-ma.github.io/blog/2022/reverseETL</id><content type="html" xml:base="https://ziqi-ma.github.io/blog/2022/reverseETL/"><![CDATA[<p>Check out this new article I wrote on Data Science@Microsoft medium blog: <a href="https://medium.com/data-science-at-microsoft/speeding-up-reverse-etl-3af04e069fd1">Speeding up “Reverse ETL”</a>.</p>]]></content><author><name></name></author><category term="blogs"/><category term="Engineering"/><summary type="html"><![CDATA[Check out this new article I wrote on Data Science@Microsoft medium blog: Speeding up “Reverse ETL”.]]></summary></entry><entry><title type="html">Navigating the (long and winding) road from innovation to production</title><link href="https://ziqi-ma.github.io/blog/2021/interview/" rel="alternate" type="text/html" title="Navigating the (long and winding) road from innovation to production"/><published>2021-12-20T16:40:16+00:00</published><updated>2021-12-20T16:40:16+00:00</updated><id>https://ziqi-ma.github.io/blog/2021/interview</id><content type="html" xml:base="https://ziqi-ma.github.io/blog/2021/interview/"><![CDATA[<p>I got to write about one of my favorite topics - “how/when research becomes useful in practice”, based on interviews with ML practitioners and organization leaders at Microsoft! It’s published on Data Science@Microsoft medium blog: <a href="https://medium.com/data-science-at-microsoft/navigating-the-long-and-winding-road-from-innovation-to-production-b9ab14ec065d">Navigating the (long and winding) road from innovation to production</a><br/></p>]]></content><author><name></name></author><category term="blogs"/><category term="Research2product"/><summary type="html"><![CDATA[I got to write about one of my favorite topics - “how/when research becomes useful in practice”, based on interviews with ML practitioners and organization leaders at Microsoft! It’s published on Data Science@Microsoft medium blog: Navigating the (long and winding) road from innovation to production]]></summary></entry><entry><title type="html">What do we talk about when we talk about ML robustness</title><link href="https://ziqi-ma.github.io/blog/2021/robustness/" rel="alternate" type="text/html" title="What do we talk about when we talk about ML robustness"/><published>2021-12-20T16:40:16+00:00</published><updated>2021-12-20T16:40:16+00:00</updated><id>https://ziqi-ma.github.io/blog/2021/robustness</id><content type="html" xml:base="https://ziqi-ma.github.io/blog/2021/robustness/"><![CDATA[<p>Check out this new article I wrote on Data Science@Microsoft medium blog: <a href="https://medium.com/data-science-at-microsoft/what-do-we-talk-about-when-we-talk-about-ml-robustness-4e58a55f9b8f">What do we talk about when we talk about ML robustness</a>. Distribution shift is only part of the story!</p>]]></content><author><name></name></author><category term="blogs"/><category term="Research2product"/><summary type="html"><![CDATA[Check out this new article I wrote on Data Science@Microsoft medium blog: What do we talk about when we talk about ML robustness. Distribution shift is only part of the story!]]></summary></entry></feed>