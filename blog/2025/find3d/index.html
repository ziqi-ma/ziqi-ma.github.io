<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Find3D: Localizing Semantic Concepts in the 3D Space | Ziqi Ma </title> <meta name="author" content="Ziqi Ma"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ziqi-ma.github.io/blog/2025/find3d/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ziqi</span> Ma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Menu </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">Blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Find3D: Localizing Semantic Concepts in the 3D Space</h1> <p class="post-meta"> Created in August 06, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   ·   <a href="/blog/category/blogs"> <i class="fa-solid fa-tag fa-sm"></i> blogs</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Intelligence in the virtual world can live in the form of language, yet the physical world is different – it is described by physical coordinates of x,y, and z. To build a bridge between the two, in the simplest sense, means being able to pinpoint where in (x,y,z) a semantic concept corresponds to. I believe that localizing semantic concepts in space is the first step towards enabling physical intelligence. This is what <a href="https://ziqi-ma.github.io/find3dsite/">Find3D</a> sets out to solve.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/teaser-480.webp 480w,/assets/img/find3dblog/teaser-800.webp 800w,/assets/img/find3dblog/teaser-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/find3dblog/teaser.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> (Generated by GPT-4o w/ human editing) </div> <h2 id="constraining-the-solution-space---key-design-decisions">Constraining the solution space - key design decisions</h2> <p>Of course, localization cannot be solved in one single project. We make a few design decisions to focus on the most critical setting:</p> <ol> <li>Work on the point cloud representation: point clouds are easily accessible with depth sensors in robotic applications, and <a href="https://arxiv.org/abs/2503.11651" rel="external nofollow noopener" target="_blank">VGGT-type methods</a> yield point maps easily. </li> <li>Focus on object parts - manipulation-type tasks require understanding below the object level - e.g. lifting the handle. Parts are closely related to affordance, yet visual understanding beyond the object granularity has been much less studied than object-level understanding. </li> <li>Semantic localization should be 3D-native rather than 2D-lifted. Why? <ol type="a"> <li>Multiview camera systems cannot scale to in-the-wild settings, moving the camera cannot scale to dynamic scenes, and single view usually contains occlusions.</li> <li>Progress in 3D reconstruction has been accelerating (such as <a href="https://arxiv.org/abs/2412.01506" rel="external nofollow noopener" target="_blank">Trellis</a>, <a href="https://arxiv.org/abs/2503.11651" rel="external nofollow noopener" target="_blank">VGGT</a>, <a href="https://arxiv.org/abs/2502.12894" rel="external nofollow noopener" target="_blank">CAST</a>), so obtaining 3D is no longer the bottleneck.</li> </ol> </li> <li>We embrace <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" rel="external nofollow noopener" target="_blank">the bitter lesson</a>, and want to go for a scalable data strategy and a scalable architecture that can keep leveraging the growing data.anularity has been much less studied than object-level understanding. </li> </ol> <h2 id="the-data-strategy">The data strategy</h2> <p>Now we know that we want a 3D native model for localizing concepts on the part level, and one that works generally (rather on a toy set of objects). The question is how. While we have small benchmarks and datasets for segmentation such as <a href="https://cs.stanford.edu/~ericyi/project_page/part_annotation/" rel="external nofollow noopener" target="_blank">ShapeNetPart</a> and <a href="https://partnet.cs.stanford.edu/" rel="external nofollow noopener" target="_blank">PartNetE</a>, they are very constrained in their domains. We do not have annotated data for general objects. As shown in the figure below, the data volume in 3D is in no way comparable with 2D, not to mention language.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/data_scale-480.webp 480w,/assets/img/find3dblog/data_scale-800.webp 800w,/assets/img/find3dblog/data_scale-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/find3dblog/data_scale.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>While we have a data shortage in 3D, we do have abundant data sourced from the Internet, which has led to incredibly powerful <a href="https://arxiv.org/abs/2410.21276" rel="external nofollow noopener" target="_blank">VLMs</a> with visual understanding capabilities in 2D. I realized that actually we can turn the localization problem into a recognition problem by building a data engine.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/dataengine-480.webp 480w,/assets/img/find3dblog/dataengine-800.webp 800w,/assets/img/find3dblog/dataengine-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/find3dblog/dataengine.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>The data engine starts by rendering unlabeled online 3D assets from Objaverse, and obtaining masks by grid-prompting SAM. We want to highlight that existing 2D “part finding” models such as groundedSAM usually misses parts, and thus by grid-prompting we can catch more comprehensive parts, especially ones that are small or seen in less common viewpoints. The goal here is not to have complete coverage for each individual object, but rather to cover a wide range of objects and parts over the full dataset. With proper filtering, we obtain a total of around 200 masks per object (from 10 views). In the next section, I will discuss how to leverage such powerful (but noisy) data.</p> <p>By highlighting parts with colored masks, we can pass these overlaid renderings to VLMs such as Gemini to recognize the name of the highlighted part, and use a CLIP-like embedding of the name as the true label for all corresponding points. These masks then get back-projected to 3D to serve as point-level annotations. Because we use language as a medium, we naturally yield diverse labels in 2 senses:</p> <ol> <li>The language description is diverse, e.g. the body of the telescope is both called a “telescope tube” and “body of a telescope”.</li> <li>We get multiple levels of granularity, rather than adhering to one set of manually-defined criteria: e.g. we get “base” of a milkshake as well as the coarser-granularity “milkshake glass”.</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/dataengine_qualitative_website-480.webp 480w,/assets/img/find3dblog/dataengine_qualitative_website-800.webp 800w,/assets/img/find3dblog/dataengine_qualitative_website-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/find3dblog/dataengine_qualitative_website.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 90%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <h2 id="training-and-scaling-the-model">Training and scaling the model</h2> <p>The data from our data engine is very diverse, yet also noisy. Two main challenges in training are seen in the image below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/challenges_training-480.webp 480w,/assets/img/find3dblog/challenges_training-800.webp 800w,/assets/img/find3dblog/challenges_training-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/find3dblog/challenges_training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 70%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <ol> <li>As much as we try to make our labels dense, there are points that cannot be covered by our viewpoints/selected masks. We need to find ways to also provide supervision on those points.</li> <li>Because our labels might come from different granularities or come under different descriptions, each point could fall under different labels (either from the same view or from overlapping views). We need to reconcile that.</li> </ol> <p>In computer vision, we have a long history of dealing with large-scale, noisy data. We follow a contrastive approach as shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/contrastive-480.webp 480w,/assets/img/find3dblog/contrastive-800.webp 800w,/assets/img/find3dblog/contrastive-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/find3dblog/contrastive.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>For each (asset, label) pair from the data engine, we pair the pooled point features corresponding to the label’s backprojected points with the actual label feature as a positive pair. We aggregate over objects to get around 3k positive pairs per batch. Architecturally, we adopt the PT3 architecture to be able to learn geometric prior over points, as shown below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/method-480.webp 480w,/assets/img/find3dblog/method-800.webp 800w,/assets/img/find3dblog/method-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/find3dblog/method.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>With this formulation, we can address the two challenges mentioned above:</p> <ol> <li>For unannotated points, given geometric priors learned by our model, they naturally share similar features with points that have similar geometric properties (i.e. points on the same sphere that are annotated).</li> <li>For points with multiple labels, their feature is pushed towards all of the supervision labels, so that at inference time we support querying at different granularities or with language descriptions emphasizing various aspects like function or material.</li> </ol> <p>The contrastive objective helps with learning from our large-scale, diverse data. We also observe good scaling behavior as we increase the number of object categories in the training data, as shown in the figure below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/scaling-480.webp 480w,/assets/img/find3dblog/scaling-800.webp 800w,/assets/img/find3dblog/scaling-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/find3dblog/scaling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 50%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <h2 id="achieving-generalization">Achieving generalization</h2> <p>Find3D is able to locate parts on diverse objects, and can even localize challenging parts (rightmost column), such as parts that are difficult to locate in 2D – the light bulb inside the lampshade (occluded almost in every viewing direction), the rim of a mug, or the sleeve of a chromatic shirt (no edge delineation).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/qualitative_new-480.webp 480w,/assets/img/find3dblog/qualitative_new-800.webp 800w,/assets/img/find3dblog/qualitative_new-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/find3dblog/qualitative_new.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 90%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>Even on smaller datasets that other methods train on, we achieve better results zero-shot in addition to being much faster.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/quantitative_res-480.webp 480w,/assets/img/find3dblog/quantitative_res-800.webp 800w,/assets/img/find3dblog/quantitative_res-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/find3dblog/quantitative_res.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>More interestingly, we are able to generalize into the real world in ways that the more recent decomposed generation methods cannot, as shown below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/find3dblog/partpacker_comp-480.webp 480w,/assets/img/find3dblog/partpacker_comp-800.webp 800w,/assets/img/find3dblog/partpacker_comp-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/find3dblog/partpacker_comp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <h2 id="whats-next">What’s next?</h2> <p>Going back to the initial motivation, we want to solve semantic part localization because it is the first step towards bridging the current intelligence in language towards the physical space delineated by spatial coordinates x,y,z. A natural next step is to make it really useful in end-to-end robotic tasks, and tackling the more general problem of robotic/reasoning interface. Stay tuned!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/cvpr25/">3D &amp; The Bitter Lesson – CVPR25 Rambling Thoughts</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ziqi Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"dropdown-publications",title:"Publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"Projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"Blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-find3d-localizing-semantic-concepts-in-the-3d-space",title:"Find3D: Localizing Semantic Concepts in the 3D Space",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/find3d/"}},{id:"post-3d-amp-the-bitter-lesson-cvpr25-rambling-thoughts",title:"3D &amp; The Bitter Lesson \u2013 CVPR25 Rambling Thoughts",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/cvpr25/"}},{id:"post-the-emerging-paradigm-of-autoregressive-vlms",title:"The emerging paradigm of autoregressive VLMs",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/multimodal/"}},{id:"post-many-facets-of-un-truth-llm-hallucination-101",title:"Many facets of un-truth: LLM hallucination 101",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/llm101/"}},{id:"post-reasoning-about-change-lessons-learned-from-building-a-near-real-time-system-for-azure-pricing",title:"Reasoning about change: Lessons learned from building a near real-time system for Azure...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/streaming/"}},{id:"post-speeding-up-quot-reverse-etl-quot",title:"Speeding up &quot;Reverse ETL&quot;",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/reverseETL/"}},{id:"post-what-do-we-talk-about-when-we-talk-about-ml-robustness",title:"What do we talk about when we talk about ML robustness",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/robustness/"}},{id:"post-navigating-the-long-and-winding-road-from-innovation-to-production",title:"Navigating the (long and winding) road from innovation to production",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/interview/"}},{id:"news-the-fair-ml-sampling-project-i-initiated-got-featured-by-pureai-researchers-explore-intelligent-sampling-of-huge-ml-datasets-to-reduce-costs-and-maintain-model-fairness-https-pureai-com-articles-2021-05-03-intelligent-ai-sampling-aspx",title:"The fair ML sampling project I initiated got featured by PureAI: [Researchers Explore...",description:"",section:"News"},{id:"news-microsoft-filed-a-patent-based-on-my-work-on-optimizing-memory-footprint-of-the-azure-pricing-system",title:"Microsoft filed a patent based on my work on optimizing memory footprint of...",description:"",section:"News"},{id:"news-moved-to-pasadena-to-start-my-phd-at-caltech-as-a-kortschak-scholar",title:"Moved to Pasadena to start my PhD at Caltech as a Kortschak Scholar!...",description:"",section:"News"},{id:"news-our-paper-quot-calibrated-uncertainty-quantification-for-operator-learning-via-conformal-prediction-quot-is-accepted-by-tmlr",title:"Our paper &quot;Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction&quot; is accepted...",description:"",section:"News"},{id:"news-check-out-our-new-work-quot-find-any-part-in-3d-quot-https-ziqi-ma-github-io-find3dsite-a-model-that-can-localize-any-part-of-any-object-based-on-any-text-query",title:"Check out our new work [&quot;Find Any Part in 3D&quot;](https://ziqi-ma.github.io/find3dsite/): a model that...",description:"",section:"News"},{id:"news-started-my-internship-at-meta-fair-perception-team",title:"Started my internship at Meta FAIR Perception team!",description:"",section:"News"},{id:"news-quot-find-any-part-in-3d-quot-https-ziqi-ma-github-io-find3dsite-is-accepted-into-iccv-2025-and-as-a-highlight-see-you-all-in-hawaii",title:"[&quot;Find Any Part in 3D&quot;](https://ziqi-ma.github.io/find3dsite/) is accepted into ICCV 2025 and as a...",description:"",section:"News"},{id:"projects-hallucination-mitigation-via-fine-grained-refinement",title:"Hallucination Mitigation via Fine-Grained Refinement",description:"Mitigating hallucinations of GPT-family models",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-distributed-data-storage-using-materialized-intermediate-partitions",title:"Distributed Data Storage using Materialized Intermediate Partitions",description:"Patent filed",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-mikros-fair-coresets-for-greener-and-more-efficient-machine-learning-training",title:"Mikros: Fair Coresets for Greener and More Efficient Machine Learning Training",description:"Sampling algorithm for fair and efficient training",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-circuit-learning-for-quantum-metrology",title:"Circuit Learning for Quantum Metrology",description:"ML for quantum sensing",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-thoreau-amp-water2cloud",title:"Thoreau &amp; Water2Cloud",description:"ML &amp; ioT for environmental monitoring",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-learning-trajectory-for-everyday-computing",title:"Learning Trajectory for Everyday Computing",description:"Designing visual block-based games for CS education",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%69%71%69%6D%61@%6F%75%74%6C%6F%6F%6B.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=MQkdBMIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ziqi-ma","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/ziqi__ma","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>