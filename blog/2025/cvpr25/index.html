<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 3D &amp; The Bitter Lesson – CVPR25 Rambling Thoughts | Ziqi Ma </title> <meta name="author" content="Ziqi Ma"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ziqi-ma.github.io/blog/2025/cvpr25/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ziqi</span> Ma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Menu </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">Blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">3D &amp; The Bitter Lesson – CVPR25 Rambling Thoughts</h1> <p class="post-meta"> Created in July 05, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   ·   <a href="/blog/category/blogs"> <i class="fa-solid fa-tag fa-sm"></i> blogs</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>One of the implications of the bitter lesson is that scaling up data and computation is far more effective than hand-designing structures. In this sense, 3D vision is a bit weird - for any end task (robotics, AR/VR, design), we do not have easily-accessible large-scale data. When we talk about 3D vision, or 3D perception, by definition we are defining a specific modular breakdown of an end application. In <a href="https://drive.google.com/file/d/1VodGljuEhBKwZIXQwN-ApH6g2wBAVAdK/view" rel="external nofollow noopener" target="_blank">Ross Girschick’s words</a>, is 3D perception a “parser”? Or in <a href="https://x.com/jon_barron/status/1916918872177054171" rel="external nofollow noopener" target="_blank">Jon Barron’s words</a>, has generative video “bitter-lessoned” 3D?</p> <p><strong>How to reconcile the bitter lesson with the data scarcity of 3D?</strong> With this big question in mind, I arrived at CVPR25 in Nashville, hoping to get more perspectives from the community. Numerous chats, posters, and workshops later, I found myself extremely excited about two answers to the above question. Since we do not have large-scale data for any 3D application for free, we have two options:</p> <ul> <li>A. Embrace some modularity - e.g. separate out perception priors (about object, motion, or physics) into a vision module that is pretrained separately from E2E tasks</li> <li>B. Build data engines that can leverage new signals that complement our existing datasets</li> </ul> <h2 id="why-3d-understanding-tasks-are-not-parsers">Why 3D understanding tasks are not “parsers”</h2> <p>While the learning from LLMs is that large-scale E2E training &gt;&gt; human-designed structure, I do think applications like robotics could benefit from some modularity with a good perception module that needn’t be bottlenecked by the scarcity of action data.</p> <p>I see all of 3D understanding tasks as ways to find the right representation of the observed world. The question is whether such representations are best learned from scaled-up robot action data or should be improved via strong visual pretraining. Both sides gave interesting opinions at the Generalization in Robotics Manipulation workshop. On the one hand, Chelsea Finn mentioned that long video sequences in robotic data already implicitly encodes information, such as 3D. On the other hand, for people who currently favor more modular approaches (<a href="https://arxiv.org/pdf/2506.01185" rel="external nofollow noopener" target="_blank">HoMeR</a>, <a href="https://arxiv.org/abs/2405.01527" rel="external nofollow noopener" target="_blank">Track2Act</a>), they wish to pursue stronger, more informative visual representations and much more data-efficient action generalization. I assume that both eventually would help us build stronger VLA models. As shown by Gemini, a good VLM backbone pre-trained on 3D tasks such as detection and multiview correspondence (<a href="https://arxiv.org/abs/2503.20020" rel="external nofollow noopener" target="_blank">Gemini Robotics ER</a>) helps the E2E action model improve both performance and generalization.</p> <p>I do believe that we should take advantage of all the vision data and 3D understanding tasks we have to obtain the best representation – an understanding of shapes, motion, and common sense. The end goal is probably to integrate all these tasks/representations into either pretraining tasks or the vision encoder – as earlier results in language (e.g. <a href="https://arxiv.org/abs/2109.01652" rel="external nofollow noopener" target="_blank">FLAN</a>) have shown, such mixing of tasks could improve both performance on existing tasks and generalization to new tasks. Along the way, we could also learn new insights about what training recipes are scalable and efficient, and gain new insights about architecture. In this sense, I do think it is valuable to work on modular visual understanding tasks and to have standalone metrics. The true lesson from language, in my opinion, is the discovery that the autoregressive generation pretraining task benefits all downstream E2E tasks, rather than solely focusing on specific E2E tasks such as translation or summarization. Before the GPT results, it was unclear that this specific unified formulation improves E2E performance of all downstream tasks, yet people spent the time and effort investigating it because they believed in the generality of the autoregressive formulation. Thus, in addition to “don’t build a parser”, I also want to say we should not “focus only on the translation metrics” but focus on finding general representations and tasks. I think the current investigation in all 3D understanding (grounding/text-3D/motion), albeit each having its own metric, helps us in the quest of a general visual representation of the world. Eventually the validity of such representations might only be measurable on large-scale projects (think GPT), but the technology buildup towards that final evaluation needs to happen gradually and even perhaps modularly.</p> <h2 id="data-engines-look-out-for-new-signals-that-enrich-existing-data">Data engines: look out for new signals that enrich existing data</h2> <p>As we are closer than ever to exhausting our available data sources, we need to start seeking other types of signals, either from humans (in less labor-intensive ways) or from other models. I define data engines broadly as systems composed of one or more models, with the goal of enriching current data with new signals. The new signals can fall broadly in two buckets: feedback, or more diverse data sources.</p> <p>The general idea of incorporating feedback into training is nothing new: that is how we initially achieved text-to-3D (<a href="https://arxiv.org/abs/2209.14988" rel="external nofollow noopener" target="_blank">DreamFusion</a>), how we accelerated supervision for foundation models like <a href="https://arxiv.org/abs/2304.02643" rel="external nofollow noopener" target="_blank">SAM</a>, and how we now approach LLM alignment (<a href="https://arxiv.org/abs/2203.02155" rel="external nofollow noopener" target="_blank">RLHF</a>) and reasoning (<a href="https://arxiv.org/abs/2411.15124" rel="external nofollow noopener" target="_blank">RLVR</a>). These are usually one-step approaches that leverage zero or one models, but I believe there is so much more room – both in coming up with feedback more creatively, and in applying it to new domains and tasks. I came across one such formulation for robotics from a workshop talk given by Ranjay Krishna. The data engine is composed of a base model and a verifier. The verifier (<a href="https://arxiv.org/abs/2410.00371" rel="external nofollow noopener" target="_blank">AHA</a> in this case) can detect robotic failure in each stage of a potentially long-horizon task. The base model rolls out a policy, the verifier verifies, and if all stages pass, the policy is added to the dataset. Without resetting the robot, a VLM proposes the next goal, and the model repeats this rollout + verification loop, theoretically infinitely. With such a verifier, we can keep “distilling” suboptimal generator models to better ones. One could also imagine that once the base model is good enough, the verifier can be directly used for RLVR.</p> <p>At this CVPR25, there was also a lot of excitement about enriching the 3D data source via reconstruction methods (e.g. <a href="https://arxiv.org/abs/2503.11651" rel="external nofollow noopener" target="_blank">VGGT</a>). Such data can be used directly to train self-supervised representations such as <a href="https://xywu.me/sonata/" rel="external nofollow noopener" target="_blank">Sonata</a>, or combined with other foundation models to provide data for understanding tasks (such as what I tried in my work, <a href="https://ziqi-ma.github.io/find3dsite/">Find3D</a>).</p> <p>The data limitation in 3D forces us to think harder – since it is not easy to train everything E2E on large data, we need to get more creative. I am extremely excited about both paths forward: making 3D perception a bit modular and “pretrainable” to save on E2E task data, as well as exploring systems that can leverage new signals to enrich our existing data.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/multimodal/">The emerging paradigm of autoregressive VLMs</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ziqi Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"dropdown-publications",title:"Publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"Projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"Blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-3d-amp-the-bitter-lesson-cvpr25-rambling-thoughts",title:"3D &amp; The Bitter Lesson \u2013 CVPR25 Rambling Thoughts",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/cvpr25/"}},{id:"post-the-emerging-paradigm-of-autoregressive-vlms",title:"The emerging paradigm of autoregressive VLMs",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/multimodal/"}},{id:"post-many-facets-of-un-truth-llm-hallucination-101",title:"Many facets of un-truth: LLM hallucination 101",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/llm101/"}},{id:"post-reasoning-about-change-lessons-learned-from-building-a-near-real-time-system-for-azure-pricing",title:"Reasoning about change: Lessons learned from building a near real-time system for Azure...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/streaming/"}},{id:"post-speeding-up-quot-reverse-etl-quot",title:"Speeding up &quot;Reverse ETL&quot;",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/reverseETL/"}},{id:"post-what-do-we-talk-about-when-we-talk-about-ml-robustness",title:"What do we talk about when we talk about ML robustness",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/robustness/"}},{id:"post-navigating-the-long-and-winding-road-from-innovation-to-production",title:"Navigating the (long and winding) road from innovation to production",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/interview/"}},{id:"news-the-fair-ml-sampling-project-i-initiated-got-featured-by-pureai-researchers-explore-intelligent-sampling-of-huge-ml-datasets-to-reduce-costs-and-maintain-model-fairness-https-pureai-com-articles-2021-05-03-intelligent-ai-sampling-aspx",title:"The fair ML sampling project I initiated got featured by PureAI: [Researchers Explore...",description:"",section:"News"},{id:"news-microsoft-filed-a-patent-based-on-my-work-on-optimizing-memory-footprint-of-the-azure-pricing-system",title:"Microsoft filed a patent based on my work on optimizing memory footprint of...",description:"",section:"News"},{id:"news-moved-to-pasadena-to-start-my-phd-at-caltech-as-a-kortschak-scholar",title:"Moved to Pasadena to start my PhD at Caltech as a Kortschak Scholar!...",description:"",section:"News"},{id:"news-our-paper-quot-calibrated-uncertainty-quantification-for-operator-learning-via-conformal-prediction-quot-is-accepted-by-tmlr",title:"Our paper &quot;Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction&quot; is accepted...",description:"",section:"News"},{id:"news-check-out-our-new-work-quot-find-any-part-in-3d-quot-https-ziqi-ma-github-io-find3dsite-a-model-that-can-localize-any-part-of-any-object-based-on-any-text-query",title:"Check out our new work [&quot;Find Any Part in 3D&quot;](https://ziqi-ma.github.io/find3dsite/): a model that...",description:"",section:"News"},{id:"news-started-my-internship-at-meta-fair-perception-team",title:"Started my internship at Meta FAIR Perception team!",description:"",section:"News"},{id:"news-quot-find-any-part-in-3d-quot-https-ziqi-ma-github-io-find3dsite-is-accepted-into-iccv-2025-and-as-a-highlight-see-you-all-in-hawaii",title:"[&quot;Find Any Part in 3D&quot;](https://ziqi-ma.github.io/find3dsite/) is accepted into ICCV 2025 and as a...",description:"",section:"News"},{id:"projects-hallucination-mitigation-via-fine-grained-refinement",title:"Hallucination Mitigation via Fine-Grained Refinement",description:"Mitigating hallucinations of GPT-family models",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-distributed-data-storage-using-materialized-intermediate-partitions",title:"Distributed Data Storage using Materialized Intermediate Partitions",description:"Patent filed",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-mikros-fair-coresets-for-greener-and-more-efficient-machine-learning-training",title:"Mikros: Fair Coresets for Greener and More Efficient Machine Learning Training",description:"Sampling algorithm for fair and efficient training",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-circuit-learning-for-quantum-metrology",title:"Circuit Learning for Quantum Metrology",description:"ML for quantum sensing",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-thoreau-amp-water2cloud",title:"Thoreau &amp; Water2Cloud",description:"ML &amp; ioT for environmental monitoring",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-learning-trajectory-for-everyday-computing",title:"Learning Trajectory for Everyday Computing",description:"Designing visual block-based games for CS education",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%69%71%69%6D%61@%6F%75%74%6C%6F%6F%6B.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=MQkdBMIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ziqi-ma","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/ziqi__ma","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>