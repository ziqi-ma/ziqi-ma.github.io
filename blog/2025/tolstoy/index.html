<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Rodney Brooks, Tolstoy, and World Models | Ziqi Ma </title> <meta name="author" content="Ziqi Ma"> <meta name="description" content="A blog post about world models and historical thinkers"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ziqi-ma.github.io/blog/2025/tolstoy/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ziqi</span> Ma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Menu </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">Blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Rodney Brooks, Tolstoy, and World Models</h1> <p class="post-meta"> Created in December 28, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   ·   <a href="/blog/category/blogs"> <i class="fa-solid fa-tag fa-sm"></i> blogs</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I first heard of “world models” before I started grad school, when a mentor mentioned Rodney Brooks’ 1991 paper <a href="https://people.csail.mit.edu/brooks/papers/representation.pdf" rel="external nofollow noopener" target="_blank">“Intelligence without representation”</a> to me in a casual discussion. Brooks raised the famous idea that “the world is its best model”, arguing against an agent maintaining an internal “world model” and favoring emergent behaviors via interacting with the environment. Fast forward 30+ years to 2025, “world models” have now become a buzzword with the release of models like <a href="https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/" rel="external nofollow noopener" target="_blank">Genie3</a> and <a href="https://marble.worldlabs.ai/" rel="external nofollow noopener" target="_blank">Marble</a>. Although we now talk about “world models” in a completely different context than Brooks, the fact that we are revisiting a core conceptual idea of AI indicates that we are at a significant transition.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tolstoy_blog/history-480.webp 480w,/assets/img/tolstoy_blog/history-800.webp 800w,/assets/img/tolstoy_blog/history-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/tolstoy_blog/history.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> Left: robots in the 1990s when Brooks discussed the ideas about “world model representations”. Right: Today’s world model: Genie3 from Google Deepmind. Images sourced from “Intelligence without representation” paper and Genie3 official website. </div> <h2 id="from-what-is-to-what-if">From “What Is” to “What If”</h2> <p>Computer vision started with perceiving: recognition, the detection and segmentation, all aiming to answer the question “what is”. Now that our perception capabilities have become good enough that we can start to ask the question of “what if”: how the environment, or the “world”, will evolve given an agent’s action. Furthermore, this is not in a man-made symbolic abstraction, but in the open world described by visual input - same as what humans see.</p> <p>This is significant because since Brook’s time, whenever we think about “what if”, or “cause and effect”, our modeling instinct kicks in, and we end up defining a model and choosing a fixed parametrization, only to find out later that it’s not general enough. Furthermore, the more structure there is, the harder to leverage large-scale, naturally-existing data (i.e. videos). If the world is represented as videos, that is general enough. How about actions?</p> <h2 id="what-is-action-from-navigation-to-manipulation">What is Action? From Navigation to Manipulation++</h2> <p>World models started with <a href="https://arxiv.org/abs/2412.03572" rel="external nofollow noopener" target="_blank">navigation</a>, and later ones like <a href="https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/" rel="external nofollow noopener" target="_blank">Genie3</a>, <a href="https://marble.worldlabs.ai/" rel="external nofollow noopener" target="_blank">Marble</a>, and <a href="https://github.com/Tencent-Hunyuan/HY-WorldPlay" rel="external nofollow noopener" target="_blank">Hunyuan-WorldPlay</a> are all heavy on navigation. The action space is thus very constrained. While some world models like <a href="https://sites.google.com/view/roboticworldmodel/home" rel="external nofollow noopener" target="_blank">RWM</a> and <a href="https://gen-irasim.github.io/" rel="external nofollow noopener" target="_blank">IRASim</a> support manipulation in the context of robotics, the action space is still not fully general. If we want to encompass all actions available from natural videos, the hairy question of “how to define actions” arises. This is a question I raised at the <a href="https://embodied-world-models.github.io/" rel="external nofollow noopener" target="_blank">“Embodied World Models for Decision Making”</a> workshop at this year’s NeurIPS. Hands? End effectors? While having application-specific action parametrization (trained on application-specific data) is an easy answer, it seems dissatisfying.</p> <p>After all, we do have general action data in natural videos. But what is an action? For humans who like to categorize and analyze (rather than just devour data), action and causation are difficult but fascinating topics. Past thinkers like Tolstoy have pondered this.</p> <p>If I pick up my phone, that is definitely my action. How about the dog running in the background? How about the leaves falling? The first domino falling? The 10th domino falling? A sudden wind? To truly categorize these, there is the concept of “initiation”: someone initiates an action; and “reaction”: the state just evolves according to the laws of physics. This delineation relates to the philosophical question about agency and free will. On the topic of actions and effects, Tolstoy has an interesting and beautiful discussion in War and Peace, via the example of a car’s motion (in analogy to the war’s development):</p> <blockquote> <p>“Wheels creak on their axles as the cogs engage one another and the revolving pulleys whirr with the rapidity of their movement but a neighbouring wheel is as quiet and motionless as though it were prepared to remain so for a hundred years; but the moment comes when the lever catches it, and obeying the impulse that wheel begins to creak, and joins in the common motion the result and aim of which are beyond its ken.” War and Peace, 1869.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tolstoy_blog/tolstoy-480.webp 480w,/assets/img/tolstoy_blog/tolstoy-800.webp 800w,/assets/img/tolstoy_blog/tolstoy-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/tolstoy_blog/tolstoy.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 50%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> Tolstoy: my favorite writer and philosopher, whose writing first prompted me to think about actions, causation, and agency in my teenage years. </div> <p>Whereas actions in games are usually abstracted into atomic primitives, actions in the natural world are not atomic nor discrete. Everything exists in a concurrent continuum - it is hard to say what is the beginning of action, what is derived, and what is the end. While we do not need to immediately jump to that level of realism in world modeling, we do need to handle more general actions that occur in real-world videos. In my opinion, the two requirements for action representation are: a) precise. The representation needs to distinguish between a 1mm difference between picking up a glass vs. breaking it). and b) general. The representation needs to encompass different embodiments, concurrent actions, and actions by agents which move in and out of the frame. Hopefully, by choosing a good, general action formulation, we can teach our world models to learn about cause and effect without needing to become philosophers ourselves.</p> <h2 id="context-and-memory">Context and Memory</h2> <p>Having worked on 3D vision, questions about consistency, state, and memory keep me up at night, as you can imagine. One can say that “3D is the memory of object permanence”. And it is true that as humans, we only see 2D in video-like form, and we have some form of non-photogenic memory that allows us to re-enter a room we have visited before, perhaps from a different entrance (and thus a different viewpoint).</p> <p>The surprising learning from LLMs has been that we don’t need a recurrent architecture for context - full attention to the context (with good retrieval/summarization) is all you need. Long-context techniques such as <a href="https://arxiv.org/abs/2006.16236" rel="external nofollow noopener" target="_blank">linear attention</a> have been developing rapidly. However, does this hold true for world models? The key difference between world models and language is that world models encompass a lot more details that might become important later on, and a conceptual understanding of a scene is not enough. For example, if you want to pick up a can that was seen on a table a while back, the exact location matters. Even if we do not rely on context and equip our world models with stateful memory (<a href="https://github.com/Tencent-Hunyuan/HY-WorldPlay" rel="external nofollow noopener" target="_blank">Hunyuan-WorldPlay</a> has an initial version of this), if the memory is by design compressed and lossy, do we think that is enough?</p> <p>I expect our world models in 1-2 years to look quite different from what we have today. The initial prototypes are impressive enough that they inspire us to think along this new direction of letting AI model “what if”. This is an extremely difficult task, since our greatest philosophers have spent lifetimes pondering over how to understand “what if” and causations of our world. It is exciting to think that the design decisions today might play a role in eventually shaping how we model the world and automatically train future agents. I am actively thinking about many of the problems mentioned above (and more), so let’s chat if you are thinking about similar problems!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/find3d/">Find3D: Localizing Semantic Concepts in the 3D Space</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ziqi Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"dropdown-publications",title:"Publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"Projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"Blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-rodney-brooks-tolstoy-and-world-models",title:"Rodney Brooks, Tolstoy, and World Models",description:"A blog post about world models and historical thinkers",section:"Posts",handler:()=>{window.location.href="/blog/2025/tolstoy/"}},{id:"post-find3d-localizing-semantic-concepts-in-the-3d-space",title:"Find3D: Localizing Semantic Concepts in the 3D Space",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/find3d/"}},{id:"post-3d-amp-the-bitter-lesson-cvpr25-rambling-thoughts",title:"3D &amp; The Bitter Lesson \u2013 CVPR25 Rambling Thoughts",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/cvpr25/"}},{id:"post-the-emerging-paradigm-of-autoregressive-vlms",title:"The emerging paradigm of autoregressive VLMs",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/multimodal/"}},{id:"post-many-facets-of-un-truth-llm-hallucination-101",title:"Many facets of un-truth: LLM hallucination 101",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/llm101/"}},{id:"post-reasoning-about-change-lessons-learned-from-building-a-near-real-time-system-for-azure-pricing",title:"Reasoning about change: Lessons learned from building a near real-time system for Azure...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/streaming/"}},{id:"post-speeding-up-quot-reverse-etl-quot",title:"Speeding up &quot;Reverse ETL&quot;",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/reverseETL/"}},{id:"post-what-do-we-talk-about-when-we-talk-about-ml-robustness",title:"What do we talk about when we talk about ML robustness",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/robustness/"}},{id:"post-navigating-the-long-and-winding-road-from-innovation-to-production",title:"Navigating the (long and winding) road from innovation to production",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/interview/"}},{id:"news-the-fair-ml-sampling-project-i-initiated-got-featured-by-pureai-researchers-explore-intelligent-sampling-of-huge-ml-datasets-to-reduce-costs-and-maintain-model-fairness-https-pureai-com-articles-2021-05-03-intelligent-ai-sampling-aspx",title:"The fair ML sampling project I initiated got featured by PureAI: [Researchers Explore...",description:"",section:"News"},{id:"news-microsoft-filed-a-patent-based-on-my-work-on-optimizing-memory-footprint-of-the-azure-pricing-system",title:"Microsoft filed a patent based on my work on optimizing memory footprint of...",description:"",section:"News"},{id:"news-moved-to-pasadena-to-start-my-phd-at-caltech-as-a-kortschak-scholar",title:"Moved to Pasadena to start my PhD at Caltech as a Kortschak Scholar!...",description:"",section:"News"},{id:"news-our-paper-quot-calibrated-uncertainty-quantification-for-operator-learning-via-conformal-prediction-quot-is-accepted-by-tmlr",title:"Our paper &quot;Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction&quot; is accepted...",description:"",section:"News"},{id:"news-check-out-our-new-work-quot-find-any-part-in-3d-quot-https-ziqi-ma-github-io-find3dsite-a-model-that-can-localize-any-part-of-any-object-based-on-any-text-query",title:"Check out our new work [&quot;Find Any Part in 3D&quot;](https://ziqi-ma.github.io/find3dsite/): a model that...",description:"",section:"News"},{id:"news-started-my-internship-at-meta-fair-perception-team",title:"Started my internship at Meta FAIR Perception team!",description:"",section:"News"},{id:"news-quot-find-any-part-in-3d-quot-https-ziqi-ma-github-io-find3dsite-is-accepted-into-iccv-2025-and-as-a-highlight-see-you-all-in-hawaii",title:"[&quot;Find Any Part in 3D&quot;](https://ziqi-ma.github.io/find3dsite/) is accepted into ICCV 2025 and as a...",description:"",section:"News"},{id:"news-sam-3d-https-ai-meta-com-sam3d-is-released-now-you-can-upload-an-image-click-on-a-few-objects-and-get-an-interactive-3d-world",title:"[SAM 3D](https://ai.meta.com/sam3d/) is released! Now you can upload an image, click on a...",description:"",section:"News"},{id:"news-steer3d-https-glab-caltech-github-io-steer3d-is-released-generative-3d-models-can-now-listen-to-your-commands-simply-edit-and-customize-the-generations-with-natural-language",title:"[Steer3D](https://glab-caltech.github.io/steer3d/) is released! Generative 3D models can now listen to your commands -...",description:"",section:"News"},{id:"projects-hallucination-mitigation-via-fine-grained-refinement",title:"Hallucination Mitigation via Fine-Grained Refinement",description:"Mitigating hallucinations of GPT-family models",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-distributed-data-storage-using-materialized-intermediate-partitions",title:"Distributed Data Storage using Materialized Intermediate Partitions",description:"Patent filed",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-mikros-fair-coresets-for-greener-and-more-efficient-machine-learning-training",title:"Mikros: Fair Coresets for Greener and More Efficient Machine Learning Training",description:"Sampling algorithm for fair and efficient training",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-circuit-learning-for-quantum-metrology",title:"Circuit Learning for Quantum Metrology",description:"ML for quantum sensing",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-thoreau-amp-water2cloud",title:"Thoreau &amp; Water2Cloud",description:"ML &amp; ioT for environmental monitoring",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-learning-trajectory-for-everyday-computing",title:"Learning Trajectory for Everyday Computing",description:"Designing visual block-based games for CS education",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%69%71%69%6D%61@%6F%75%74%6C%6F%6F%6B.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=MQkdBMIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ziqi-ma","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/ziqi__ma","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>