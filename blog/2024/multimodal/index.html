<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The emerging paradigm of autoregressive VLMs | Ziqi Ma </title> <meta name="author" content="Ziqi Ma"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ziqi-ma.github.io/blog/2024/multimodal/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ziqi</span> Ma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Menu </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">Blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">The emerging paradigm of autoregressive VLMs</h1> <p class="post-meta"> Created in November 26, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   ·   <a href="/blog/category/blogs"> <i class="fa-solid fa-tag fa-sm"></i> blogs</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>When the <a href="https://arxiv.org/abs/2108.07258" rel="external nofollow noopener" target="_blank">“foundation models” paper</a> first used the term “paradigm shift” in 2021, I remember chuckling at how self-important the term seemed. Today in the end 2024, I am feeling the full gravity of the shift from building problem-specific custom solutions to building upon a common core component that can scale. I think this change deserves to be called a “paradigm shift”, true to Kuhn’s original intent for the term in 1962. This shift is enabled by the emergence of unified task interfaces and the convergence of model architecture. As a result, a framework of execution is also starting to consolidate, with a focus on data acquisition and systematic upscaling. I discuss each aspect in order.</p> <p><br></p> <h2 id="the-unifying-task-interface-of-autoregressive-generation">The unifying task interface of autoregressive generation</h2> <p>The early versions of modern, large-scale VLMs are trained contrastively (<a href="https://arxiv.org/abs/2103.00020" rel="external nofollow noopener" target="_blank">CLIP</a>), perhaps because image-text pairs are the easiest type of supervision that we can obtain at scale. Soon we realized that making image and text features similar is not sufficient. With a similarity comparison we can only do retrieval and classification, but we dream of something more – features from another modality that is not only similar but also usable – to reason, to generate, to answer questions. These tasks can all be unified under the autoregressive generation framework – and thus, trained on the same language modeling loss. Earlier models like <a href="https://arxiv.org/abs/2205.01917" rel="external nofollow noopener" target="_blank">CoCa</a>, <a href="https://arxiv.org/abs/2301.12597" rel="external nofollow noopener" target="_blank">BLIP2</a>, <a href="https://arxiv.org/abs/2111.08276" rel="external nofollow noopener" target="_blank">X-VLM</a> use a hybrid of contrastive and language modeling objectives. The common benchmarks (<a href="https://arxiv.org/abs/1612.00837" rel="external nofollow noopener" target="_blank">VQAV2</a>, <a href="https://lil.nlp.cornell.edu/nlvr/" rel="external nofollow noopener" target="_blank">NLVR2</a>) are old, yet the unification of tasks is new. Later models (<a href="https://arxiv.org/abs/2204.14198" rel="external nofollow noopener" target="_blank">Flamingo</a>, <a href="https://arxiv.org/abs/2209.06794" rel="external nofollow noopener" target="_blank">PALI family</a>, <a href="https://arxiv.org/abs/2208.10442" rel="external nofollow noopener" target="_blank">BEiT3</a>, <a href="https://arxiv.org/abs/2304.08485" rel="external nofollow noopener" target="_blank">Llava</a> etc.) discard the contrastive component and fully use the language modeling objective. While one might argue that putting language-centric tasks into an autoregressive framework is nothing surprising, we also see an increased coverage of more vision-centric tasks.</p> <p>From the early exploration of <a href="https://arxiv.org/abs/2102.12092" rel="external nofollow noopener" target="_blank">DALLE-1</a>, several lines of work (<a href="https://sites.research.google/parti/" rel="external nofollow noopener" target="_blank">Parti</a> – perhaps <a href="https://arxiv.org/abs/2312.11805" rel="external nofollow noopener" target="_blank">Gemini</a>, <a href="https://arxiv.org/abs/2405.09818" rel="external nofollow noopener" target="_blank">Chameleon</a> family) also formulate image generation as an autoregressive task under the same interface. Although diffusion is still the leading solution in many settings, it is not as unified with all the other tasks, and people seem to be still pushing on autoregressive generation.</p> <p>Denser vision tasks are first formulated as more fine-grained language tasks, such as <a href="https://arxiv.org/abs/2210.01936" rel="external nofollow noopener" target="_blank">ARO</a>, <a href="https://arxiv.org/abs/2207.00221" rel="external nofollow noopener" target="_blank">VL-Checklist</a> and <a href="https://arxiv.org/abs/2204.03162" rel="external nofollow noopener" target="_blank">Winoground</a>. Such tasks seem more as an evaluation (or a highlight of current failure modes) and not training tasks. However, more recently, classical dense vision tasks like detection and segmentation are also formulated autoregressively with additional tokens denoting boxes and masks. These tasks are incorporated into pre-training for models like the PALI family, yielding decent performance and enabling considerable improvement on a variety of tasks. <a href="https://arxiv.org/abs/2406.16860" rel="external nofollow noopener" target="_blank">Cambrian1</a> repurposed most vision tasks into a language framework, including detection, segmentation, and depth (formulated as comparison). Formulating dense vision tasks autoregressively (especially the segmentation masks which first start with polygon coordinates then use the <a href="https://arxiv.org/abs/1711.00937" rel="external nofollow noopener" target="_blank">VQ-VAE</a> encoding) is mind-blowing for me, yet the performance seems okay. More recently, <a href="https://arxiv.org/abs/2409.17146" rel="external nofollow noopener" target="_blank">Molmo</a> releases a “pointing” feature where a VLM can answer a question by “pointing” in an image.</p> <div class="row mt-4"> <div class="col-sm mt-5 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vlmblog/paligemma-480.webp 480w,/assets/img/vlmblog/paligemma-800.webp 800w,/assets/img/vlmblog/paligemma-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/vlmblog/paligemma.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-5 mt-md-0"> <div style="display: flex; margin: auto; justify-content: center; vertical-align: middle;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vlmblog/molmo-480.webp 480w,/assets/img/vlmblog/molmo-800.webp 800w,/assets/img/vlmblog/molmo-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/vlmblog/molmo.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> How PaliGemma and Molmo formulate dense vision tasks autoregressively. </div> <p>A unified interface is in no ways prescriptive of all future models. However, it proposes one way to push ahead on scale, and one way to let the “divide, iterate, conquer” kick off. For example, <a href="https://arxiv.org/abs/2406.16860" rel="external nofollow noopener" target="_blank">Cambrian1</a> uses a common pipeline to measure the effectiveness of various types of visual encoders. While the comparison certainly comes with its bias, I think this type of interface abstraction is the right way for a large team to make progress on a large foundation model.</p> <p><br></p> <h2 id="the-architecture-convergence">The architecture convergence</h2> <p>Architecture co-develops with the tasks. It is hard to say which causes which - the unification of tasks under the autoregressive framework, or the scalability of the decoder architecture. Since the output eventually is tokens (language, image or special tokens), the question becomes how to map raw image input to the token space. A few attempts have been cross-attention (<a href="https://arxiv.org/abs/2204.14198" rel="external nofollow noopener" target="_blank">Flamingo</a>), adapter with learnable queries (<a href="https://arxiv.org/abs/2204.14198" rel="external nofollow noopener" target="_blank">Flamingo</a>, <a href="https://arxiv.org/abs/2301.12597" rel="external nofollow noopener" target="_blank">BLIP-2</a> QFormer), or simple projection (<a href="https://arxiv.org/abs/2407.07726" rel="external nofollow noopener" target="_blank">PaliGemma</a>). For the early-fusion models (<a href="https://sites.research.google/parti/" rel="external nofollow noopener" target="_blank">Parti</a>, <a href="https://arxiv.org/abs/2405.09818" rel="external nofollow noopener" target="_blank">Chameleon</a> family), the visual signal is tokenized via some self-supervised discrete tokenizer such as <a href="https://arxiv.org/abs/1711.00937" rel="external nofollow noopener" target="_blank">VQ-VAE</a> or <a href="https://arxiv.org/abs/2303.13450" rel="external nofollow noopener" target="_blank">Set-a-Scene</a>.</p> <p>A natural decomposition becomes an image encoder, an adapter mechanism (one of the above three) which maps the image into a “soft prompt”, and a language decoder. Earlier works have attempted to train these encoders from scratch (<a href="https://arxiv.org/abs/2205.01917" rel="external nofollow noopener" target="_blank">CoCa</a>, <a href="https://arxiv.org/abs/2111.08276" rel="external nofollow noopener" target="_blank">X-VLM</a>, <a href="https://arxiv.org/abs/2208.10442" rel="external nofollow noopener" target="_blank">BEiT3</a>), yet later works (<a href="https://arxiv.org/abs/2209.06794" rel="external nofollow noopener" target="_blank">PALI</a> family, <a href="https://arxiv.org/abs/2405.09818" rel="external nofollow noopener" target="_blank">Chameleon</a>, <a href="https://arxiv.org/abs/2112.04482" rel="external nofollow noopener" target="_blank">Flava</a>, etc.) mainly leverage existing models as encoders, partly due to the increased size of these models. Somewhat surprisingly, CLIP-type image-text contrastively learned vision encoders seem to work better than vision-only encoders (<a href="https://arxiv.org/abs/2406.16860" rel="external nofollow noopener" target="_blank">Cambrian1</a>), even on vision-centric tasks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vlmblog/paligemma_arch-480.webp 480w,/assets/img/vlmblog/paligemma_arch-800.webp 800w,/assets/img/vlmblog/paligemma_arch-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/vlmblog/paligemma_arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> Architecture of PaliGemma. The architectures of autoregressive VLMs seem to simplify over time. Not much bells and whistles, just a vision encoder, a language encoder, then autoregressive generation. Usually pretrained encoders are used. For image generation, latent tokens are usually first generated, then mapped back to pixel space. </div> <p>The question on architecture, under the autoregressive framework’s emerging interfaces, now becomes “how to encode and tokenize image”. This is a much more constrained design space, yet one that allows for relatively smooth interface with all the existing language architecture and the tasks that have been subsumed under this framework. Any new model proposal can be easily plugged in and swapped out.</p> <p><br></p> <h2 id="data">Data</h2> <p>As the architectures are converging, attention has been turning to data and pretraining task mixture. To improve performance in specific domains like OCR or dense visual tasks like detection and segmentatino, such tasks are sometimes directly added into the pretraining mix. While re-mixing existing data is a quick fix, we really do need to think hard about what new kinds of data can enable new capabilities or big improvements.</p> <p>Throughout the history of AI, data has usually co-developed with the architecture. On the one hand, the availability of supervision at scale greatly determines what models are possible to train. For example, image-text pairs naturally gave rise to contrastive models. Interleaved HTML text (<a href="https://arxiv.org/abs/2201.07520" rel="external nofollow noopener" target="_blank">CM3</a>) at scale enables mixed-modality output models. Dense datasets like <a href="https://cocodataset.org/#home" rel="external nofollow noopener" target="_blank">COCO</a> (and the plethora of datasets built upon it) enable models that have any ability to reason in a denser/object-centric manner. On the other hand, smaller-scale POCs determine what types of data is worth investing on, as one can see in the lines of industrial work that either get scaled up or become discontinued.</p> <p>One good example of new form of data is <a href="(https://arxiv.org/abs/2409.17146)">PixMo-Points</a>. The point dataset with 2.3M Q&amp;A pairs and 428k images enables Molmo’s new pointing feature. Their data collection via speaking and automatic speech-to-text transcription also greatly improves annotation efficiency. Meta also released a dense, mask-level annotated dataset called <a href="https://arxiv.org/abs/2312.08578" rel="external nofollow noopener" target="_blank">DCI</a> this summer, although only at a 7k-image scale. Such a dataset, if scaled up, has the potential of greatly improving grounding.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vlmblog/data-480.webp 480w,/assets/img/vlmblog/data-800.webp 800w,/assets/img/vlmblog/data-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/vlmblog/data.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> Examples of dense, mask-level captions from Meta's DCI dataset. This is exactly the kind of information that humans can get from the visual modality that would be so inconvenient to convey in language. These captions are valuable for grounding tasks, although we currently only have 7k images openly available. </div> <p><br></p> <h2 id="what-kind-of-mechanistic-investigations-do-we-need">What kind of mechanistic investigations do we need</h2> <p>With much of the community interested in the common goal of building a good multitask VLM, there is an increasing need of systematic understanding of what might work and what might not. While in old-fashioned ML works there is usually a small section reserved for theoretical derivation that is supposed to serve this purpose, in the current day we need a different kind of mechanistic understanding. The focus should be to guide the investment of time and compute, understand what scales and what does not. Good examples include <a href="https://arxiv.org/abs/2405.02246" rel="external nofollow noopener" target="_blank">Idefics2</a> and <a href="https://arxiv.org/abs/2406.16860" rel="external nofollow noopener" target="_blank">Cambrian1</a>, which define and explore the design space and scaling laws, so that not everyone needs to repeat the same experiments before making their own design decisions. While seeing so much emphasis on scaling in research is surprising, this heralds that something is on the horizon and concerted effort is forming within the community.</p> <p><br></p> <h2 id="whats-next">What’s next?</h2> <p>The forming of one set of interfaces does not mean we already nailed down most of the problems. On the contrary, it formulates one design space which can help guide us in tackling future problems in a more structured way.</p> <ul> <li>While the autoregressive framework has promise to scale, this does not mean exploration on other fronts e.g. diffusion should stop. Perhaps one could think more along the lines of unification of the two. Especially, diffusion strikes me as a much more natural way to model dense tasks.</li> <li>Going sub-object level – the main difference between the image and the language modality, in my opinion, is the rich details that the vision modality embodies. Up to the object level, one can imagine some level of abstraction into a much lower-dimension token space, yet we can perceive sub-object details on different levels of granularity, depending on the context. This is not yet well-formulated either in task or in architecture. The current way of tokenizing might or might not be sufficient.</li> <li>What are good ways to encode 3D data (for any representation), and what are good task formulations that can represent the needs for embodied applications well, while remaining compatible to the existing scalable framework?</li> </ul> <p>To these ends, I did my own investigation on generating dense semantic features for 3D point clouds, called <a href="https://arxiv.org/abs/2411.13550" rel="external nofollow noopener" target="_blank">Find3D</a>. Segmentation/localization is a straightforward use case. How to leverage this now-acquired dense knowledge to enable better reasoning? How does this representation fit into the broader VLM frameworks? These are questions that remain to be solved.</p> <p>In a sense, with the interfaces emerging, the line between free research and product roadmaps become more blurred. Coordinated, large-scale efforts are now possible. We are seeing enough initial evidence that if researchers combine efforts and work under compatible interfaces, our collective impacts could add up to something quite exciting!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/llm101/">Many facets of un-truth: LLM hallucination 101</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Ziqi Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"dropdown-publications",title:"Publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"Projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"Blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-the-emerging-paradigm-of-autoregressive-vlms",title:"The emerging paradigm of autoregressive VLMs",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/multimodal/"}},{id:"post-many-facets-of-un-truth-llm-hallucination-101",title:"Many facets of un-truth: LLM hallucination 101",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/llm101/"}},{id:"post-reasoning-about-change-lessons-learned-from-building-a-near-real-time-system-for-azure-pricing",title:"Reasoning about change: Lessons learned from building a near real-time system for Azure...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/streaming/"}},{id:"post-speeding-up-quot-reverse-etl-quot",title:"Speeding up &quot;Reverse ETL&quot;",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/reverseETL/"}},{id:"post-what-do-we-talk-about-when-we-talk-about-ml-robustness",title:"What do we talk about when we talk about ML robustness",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/robustness/"}},{id:"post-navigating-the-long-and-winding-road-from-innovation-to-production",title:"Navigating the (long and winding) road from innovation to production",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/interview/"}},{id:"news-the-fair-ml-sampling-project-i-initiated-got-featured-by-pureai-researchers-explore-intelligent-sampling-of-huge-ml-datasets-to-reduce-costs-and-maintain-model-fairness-https-pureai-com-articles-2021-05-03-intelligent-ai-sampling-aspx",title:"The fair ML sampling project I initiated got featured by PureAI: [Researchers Explore...",description:"",section:"News"},{id:"news-microsoft-filed-a-patent-based-on-my-work-on-optimizing-memory-footprint-of-the-azure-pricing-system",title:"Microsoft filed a patent based on my work on optimizing memory footprint of...",description:"",section:"News"},{id:"news-moved-to-pasadena-to-start-my-phd-at-caltech-as-a-kortschak-scholar",title:"Moved to Pasadena to start my PhD at Caltech as a Kortschak Scholar!...",description:"",section:"News"},{id:"news-our-paper-quot-calibrated-uncertainty-quantification-for-operator-learning-via-conformal-prediction-quot-is-accepted-by-tmlr",title:"Our paper &quot;Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction&quot; is accepted...",description:"",section:"News"},{id:"news-check-out-our-new-work-quot-find-any-part-in-3d-quot-https-ziqi-ma-github-io-find3dsite-a-model-that-can-localize-any-part-of-any-object-based-on-any-text-query",title:"Check out our new work [&quot;Find Any Part in 3D&quot;](https://ziqi-ma.github.io/find3dsite/): a model that...",description:"",section:"News"},{id:"projects-hallucination-mitigation-via-fine-grained-refinement",title:"Hallucination Mitigation via Fine-Grained Refinement",description:"Mitigating hallucinations of GPT-family models",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-distributed-data-storage-using-materialized-intermediate-partitions",title:"Distributed Data Storage using Materialized Intermediate Partitions",description:"Patent filed",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-mikros-fair-coresets-for-greener-and-more-efficient-machine-learning-training",title:"Mikros: Fair Coresets for Greener and More Efficient Machine Learning Training",description:"Sampling algorithm for fair and efficient training",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-circuit-learning-for-quantum-metrology",title:"Circuit Learning for Quantum Metrology",description:"ML for quantum sensing",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-thoreau-amp-water2cloud",title:"Thoreau &amp; Water2Cloud",description:"ML &amp; ioT for environmental monitoring",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-learning-trajectory-for-everyday-computing",title:"Learning Trajectory for Everyday Computing",description:"Designing visual block-based games for CS education",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%69%71%69%6D%61@%6F%75%74%6C%6F%6F%6B.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=MQkdBMIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ziqi-ma","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/ziqi__ma","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>